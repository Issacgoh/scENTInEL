{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brave-suicide",
   "metadata": {},
   "source": [
    "# Key concepts behind SGD pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-findings",
   "metadata": {},
   "source": [
    "# Understanding the Normalized Adjacency Matrix in SGD PageRank\n",
    "\n",
    "In the presented computation, a distinct form of the normalized adjacency matrix has been employed. While this is not the traditional Laplacian, such normalized matrices have become increasingly prevalent, especially in the domain of graph-based learning methodologies, like graph convolutional networks (GCNs).\n",
    "\n",
    "## 1. **Degrees Matrix**:\n",
    "\n",
    "For each node in the graph, the sum of the weights of its edges is computed. This calculation is a generalization of the degree concept for weighted graphs. The formula for the degree of a node \\(i\\) in a weighted graph is given by:\n",
    "\n",
    "$$ \\text{degree}(i) = \\sum_j \\text{weight}(i, j) $$\n",
    "\n",
    "\n",
    "Additionally, adding \\(1\\) to each node's degree represents the inclusion of a self-loop, ensuring every node retains a connection to itself.\n",
    "\n",
    "## 2. **Inverse Square Root of Degrees**:\n",
    "\n",
    "The inverse of the square root of the degrees is computed. This transformation is pivotal for normalization. The main objective behind this is to ensure nodes with a high degree don't exert a disproportionate influence. The formula to compute this for a node \\(i\\) is:\n",
    "\n",
    "$$ \\text{inv_sqrt_degree}(i) = \\frac{1}{\\sqrt{\\text{degree}(i)}} $$\n",
    "\n",
    "\n",
    "## 3. **Normalized Matrix**:\n",
    "\n",
    "The final normalized matrix is created by multiplying the inverse square root diagonal matrix with the adjacency matrix, and then with the inverse square root matrix again:\n",
    "\n",
    "$$ \\text{normalized_matrix} = \\text{inv_sqrt_deg_matrix} \\times \\text{neighborhood_matrix} \\times \\text{inv_sqrt_deg_matrix} $$\n",
    "\n",
    "This process yields a symmetrically normalized adjacency matrix.\n",
    "\n",
    "## **Implications of the Normalized Adjacency Matrix**:\n",
    "\n",
    "- **Highlighting Local Structures**: The resulting PageRank scores derived using this matrix accentuate local structures more than a traditional approach. As such, nodes embedded within closely-knit communities or cliques might receive higher scores in this method as opposed to traditional PageRank.\n",
    "  \n",
    "- **Equitable Distribution**: The normalization mechanism ensures that nodes with an extremely high degree won't dominate the computation. This normalization ensures that there's a more balanced sharing of importance across nodes.\n",
    "\n",
    "- **Connectivity Over Weight**: By using the normalized matrix, the emphasis is placed more on the patterns of connectivity rather than the raw edge weights. This means that the focus is more on the relational dynamics between nodes and their surrounding neighborhoods.\n",
    "\n",
    "## **Understanding the Outcomes of SGD PageRank with the Normalized Adjacency Matrix**:\n",
    "\n",
    "1. **Local and Global Importance**: The PageRank scores derived can be viewed as a balance between global significance and local neighborhood relevance. Nodes in tight communities might receive elevated importance due to the amplified local structure, while still maintaining a connection to the broader graph structure.\n",
    "\n",
    "2. **Resilience to High-Degree Nodes**: Due to the symmetrical normalization, nodes with a high number of connections don't overshadow the computation. This mechanism ensures every node gets a fair consideration based on its connectivity patterns.\n",
    "\n",
    "In summary, the integration of a normalized adjacency matrix in the SGD PageRank algorithm paves the way for a nuanced derivation of node importance. This approach takes into account both the global structure of the graph and the intricate dynamics of local neighborhoods.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-recipient",
   "metadata": {},
   "source": [
    "## **Conclusions:**\n",
    "\n",
    "1. **Using Normalized Laplacian:**\n",
    "By using the normalized Laplacian in the context of single-cell data, you're essentially down-weighting the influence of cells that are highly connected (i.e., cells that are similar to many other cells). This can be beneficial if you believe that highly connected cells are not necessarily more \"important\" than less connected cells. By reducing their influence, you might highlight more unique or specialized cell states that could be overshadowed in a traditional PageRank computation.\n",
    "\n",
    "2. **Interpretability and Biological Relevance:**\n",
    "The key question is whether the rank scores, and the resulting ordering of cells, have biological relevance or offer insights into the cellular landscape. For example, do high-ranked cells correspond to critical transitional states in a developmental trajectory? Or do they highlight rare cell populations with distinct gene expression profiles?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-scotland",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent PageRank (SGD PageRank)\n",
    "\n",
    "This document provides a detailed algorithmic description for a SGD PageRank implementation tailored for graphs derived from single-cell multiomic data. The aim is to prioritize the nodes (cells) based on their topological importance, refined by SGD to handle large-scale data efficiently.\n",
    "\n",
    "## Preliminary Steps\n",
    "\n",
    "### Dynamic Neighborhood Hopping\n",
    "\n",
    "To capture the extended topological features of the graph, we implement dynamic neighborhood hopping:\n",
    "\n",
    "$$\n",
    "A^{(\\alpha)} = A^{\\alpha}\n",
    "$$\n",
    "\n",
    "where \\( A \\) is the adjacency matrix, and \\( \\alpha \\) is the predefined number of hops ensuring all nodes have direct paths to all other nodes within \\( \\alpha \\) hops.\n",
    "\n",
    "### Scaling Factor Calculation (Si)\n",
    "\n",
    "The scaling factor for each node is calculated to down-weight the influence of highly connected nodes:\n",
    "\n",
    "$$\n",
    "S_i = \\frac{1}{\\text{degree}(i)} + C(D_i)\n",
    "$$\n",
    "\n",
    "where \\( C(D_i) \\) represents a correction based on the node's adjacency set.\n",
    "\n",
    "### Matrix Scaling (Mij)\n",
    "\n",
    "We scale the k-nearest neighbors (KNN) matrix by applying a dot product of \\( S_i \\) to both incoming and outgoing connections:\n",
    "\n",
    "$$\n",
    "M_{ij} = S_i \\cdot M \\cdot S_i\n",
    "$$\n",
    "\n",
    "where \\( M \\) is the KNN matrix.\n",
    "\n",
    "## SGD PageRank Algorithm\n",
    "\n",
    "The main algorithm proceeds as follows:\n",
    "\n",
    "```python\n",
    "def SGDpagerank(M, num_iterations, mini_batch_size, initial_learning_rate, tolerance, d, full_batch_update_iters, dip_window, plateau_iterations, sampling_method, init_vect=None, **kwargs):\n",
    "    # ... (implementation details)\n",
    "```\n",
    "\n",
    "### Learning Rate (Alpha)\n",
    "\n",
    "The learning rate is updated at each iteration to ensure convergence:\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{1}{1 + \\text{decay_rate} \\cdot \\text{iteration}}\n",
    "$$\n",
    "\n",
    "### PageRank Initialization\n",
    "\n",
    "A random rank vector \\( v \\) is initialized and normalized:\n",
    "\n",
    "$$\n",
    "v = \\frac{\\text{rand}(N, 1)}{\\| \\text{rand}(N, 1) \\|_1}\n",
    "$$\n",
    "\n",
    "### Mini-Batch SGD Iterations\n",
    "\n",
    "At each iteration, a subset of nodes is selected, and the PageRank vector is updated:\n",
    "\n",
    "$$\n",
    "v_{\\text{mini_batch}} = d \\cdot (\\alpha \\cdot M_{\\text{mini_batch}} @ v) + \\left(\\frac{1 - d}{N}\\right)\n",
    "$$\n",
    "\n",
    "where \\( @ \\) denotes matrix-vector multiplication.\n",
    "\n",
    "## Convergence Check\n",
    "\n",
    "We monitor the L2 norm of the PageRank vector difference for convergence:\n",
    "\n",
    "$$\n",
    "\\| v_{\\text{iter}} - v_{\\text{prev}} \\|_2 < \\text{tolerance}\n",
    "$$\n",
    "\n",
    "## Full-Batch Updates\n",
    "\n",
    "After the main SGD iterations, we perform a number of full-batch updates for fine-tuning:\n",
    "\n",
    "$$\n",
    "v = d \\cdot (M @ v) + \\left(\\frac{1 - d}{N}\\right)\n",
    "$$\n",
    "\n",
    "## Post-Processing\n",
    "\n",
    "### Softmax Transformation\n",
    "\n",
    "Once the PageRank vector is obtained, a softmax transformation is applied to obtain a probability distribution for sampling:\n",
    "\n",
    "$$\n",
    "P_i = \\frac{e^{v_i}}{\\sum_{j=1}^N e^{v_j}}\n",
    "$$\n",
    "\n",
    "### Sampling Strategy\n",
    "\n",
    "Finally, we perform sampling from the softmax-transformed PageRank scores to select nodes:\n",
    "\n",
    "- Sampling is done 100 times using the PageRank scores.\n",
    "- The observed probability of sampling is used to derive the output node indices at a given proportion.\n",
    "\n",
    "This concludes the detailed algorithmic description of the SGD PageRank methodology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-dream",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "workhorse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
