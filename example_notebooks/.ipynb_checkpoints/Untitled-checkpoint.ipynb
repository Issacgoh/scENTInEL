{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "second-dimension",
   "metadata": {},
   "source": [
    "# SCENTInEL\n",
    "#### sc ElasticNet transductive and inductive ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-violation",
   "metadata": {},
   "source": [
    "# LR multi-tissue cross-comparison\n",
    "\n",
    "##### Ver:: A2_V6\n",
    "##### Author(s) : Issac Goh\n",
    "##### Date : 220823;YYMMDD\n",
    "### Author notes\n",
    "    - Current defaults scrpae data from web, so leave as default and run\n",
    "    - slices model and anndata to same feature shape, scales anndata object\n",
    "    - added some simple benchmarking\n",
    "    - creates dynamic cutoffs for probability score (x*sd of mean) in place of more memory intensive confidence scoring\n",
    "    - Does not have majority voting set on as default, but module does exist\n",
    "    - Multinomial logistic relies on the (not always realistic) assumption of independence of irrelevant alternatives whereas a series of binary logistic predictions does not. collinearity is assumed to be relatively low, as it becomes difficult to differentiate between the impact of several variables if this is not the case\n",
    "    - Feel free to feed this model latent representations which capture non-linear relationships, the model will attempt to resolve any linearly seperable features. Feature engineering can be applied here.\n",
    "    \n",
    "### Features to add\n",
    "    - Add ability to consume anndata zar format for sequential learning\n",
    "### Modes to run in\n",
    "    - Run in training mode\n",
    "    - Run in projection mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-locking",
   "metadata": {},
   "source": [
    "# Parameter inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "'pan_fetal':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/adifa_lr/celltypist_model.Pan_Fetal_Human.pkl',\n",
    "'pan_fetal_wget':'https://celltypist.cog.sanger.ac.uk/models/Pan_Fetal_Suo/v2/Pan_Fetal_Human.pkl',\n",
    "'adata_scvi':'/nfs/team205/ig7/mount/gdrive/g_cloud/projects/amniontic_fluid/scvi_low_dim_model.sav',\n",
    "'adata_ldvae':'/nfs/team205/ig7/mount/gdrive/g_cloud/projects/amniontic_fluid/ldvae_low_dim_model.sav',\n",
    "'adata_harmony':'/nfs/team205/ig7/work_backups/backup_210306/projects/amiotic_fluid/train_low_dim_model/organ_low_dim_model.sav',\n",
    "'test_low_dim_ipsc_ys':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_030522_notebooks/Integrating_HM_data_030522/YS_logit/lr_model.sav',\n",
    "'YS_X':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/resources/YS_X_model_080922.sav',\n",
    "'YS_X_V3':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/train_YS_full_X_model/YS_X_A2_V12_lvl3_ELASTICNET_YS.sav',\n",
    "'SK_model':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/LR_app_format/hudaa_skin/for_hudaa_A1_V2',\n",
    "'Hudaa_model_trained':'/nfs/team298/hg6/Fetal_skin/LR_15012023/train-all_model.pkl',\n",
    "\n",
    "}\n",
    "\n",
    "adatas_dict = {\n",
    "'Fetal_skin_raw': '/nfs/team298/hg6/Fetal_skin/data/FS_raw_sub.h5ad',\n",
    "'vascular_organoid': '/nfs/team298/hg6/Fetal_skin/data/vasc_org_raw.h5ad',\n",
    "'YS':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V5_scvi_YS_integrated/A2_V5_scvi_YS_integrated_raw_qc_scr_umap.h5ad',\n",
    "'YS_test':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/LR_app_format/ys_test_data.h5ad',\n",
    "'YS_A2_V10_X_raw':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V10_scvi_YS_integrated/A2_V10_raw_counts_full_no_obs.h5ad',\n",
    "'YS_A2_V10_X':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V10_scvi_YS_integrated/A2_V10_qc_raw.h5ad'\n",
    "}\n",
    "\n",
    "# Variable assignment\n",
    "train_model = False\n",
    "feat_use = 'joint_annotation_20220202'\n",
    "adata_key = 'Fetal_skin_raw'#'fliv_wget_test' # key for dictionary entry containing local or web path to adata/s can be either url or local \n",
    "data_merge = False # read and merge multiple adata (useful, but keep false for now)\n",
    "model_key = 'SK_model'#'test_low_dim_ipsc_ys'# key for model of choice can be either url or local \n",
    "train_x_partition = 'X' # what partition was the data trained on? To keep simple, for now only accepts 'X'\n",
    "dyn_std = 1.96 # Dynamic cutoffs using std of the mean for each celltype probability, gives a column notifying user of uncertain labels 1 == 68Ci, 1.96 = 95CI\n",
    "freq_redist = 'joint_annotation_20220202'#'cell.labels'#'False#'cell.labels'#False # False or key of column in anndata object which contains labels/clusters // not currently implemented\n",
    "partial_scale = True # should data be scaled in batches?\n",
    "QC_normalise = True # should data be normalised?\n",
    "\n",
    "# training variables\n",
    "penalty='elasticnet' # can be [\"l1\",\"l2\",\"elasticnet\"]\n",
    "sparcity=0.5 # C penalty for degree of regularisation\n",
    "thread_num = -1\n",
    "l1_ratio = 0.5 # ratio between L1 and L2 regulrisatiuon depending on penatly method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-wealth",
   "metadata": {},
   "source": [
    "# Read in query data for projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key, QC_normalise)\n",
    "    print('adata_loaded')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    display_cpu = DisplayCPU()\n",
    "    display_cpu.start()\n",
    "    try:\n",
    "        model_trained = prep_training_data(feat_use = feat_use,\n",
    "        adata_temp = adata,\n",
    "        train_x_partition = train_x_partition,\n",
    "        model_key = model_key + '_lr_model',\n",
    "        batch_correction = 'Harmony',\n",
    "        var_length = 7500,\n",
    "        batch_key = 'donor',\n",
    "        penalty='elasticnet', # can be [\"l1\",\"l2\",\"elasticnet\"],\n",
    "        sparcity=sparcity, #If using LR without optimisation, this controls the sparsity in model\n",
    "        max_iter = 1000, #Increase if experiencing max iter issues\n",
    "        l1_ratio = l1_ratio, #If using elasticnet without optimisation, this controls the ratio between l1 and l2)\n",
    "        partial_scale = False, #partial_scale,\n",
    "        tune_hyper_params = True # Current implementation is very expensive, intentionally made rigid for now\n",
    "        )\n",
    "        filename =model_name\n",
    "        pkl.dump(model_trained, open(filename, 'wb'))\n",
    "    finally: #\n",
    "        current, peak = display_cpu.stop()\n",
    "        t1 = time.time()\n",
    "        time_s = t1-t0\n",
    "        print('training complete!')\n",
    "        time.sleep(3)\n",
    "        print('projection time was ' + str(time_s) + ' seconds')\n",
    "        print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "        print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "        print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "        print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))\n",
    "    model_lr= model_trained\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key)\n",
    "else:\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "    model = load_models(models,model_key)\n",
    "    model_lr =  model\n",
    "    \n",
    "# run with usage logger\n",
    "import time\n",
    "t0 = time.time()\n",
    "display_cpu = DisplayCPU()\n",
    "display_cpu.start()\n",
    "try: #code here ##\n",
    "    pred_out,train_x,model_lr,adata_temp = reference_projection(adata, model_lr, dyn_std,partial_scale)\n",
    "    if freq_redist != False:\n",
    "        pred_out = freq_redist_68CI(adata,freq_redist)\n",
    "        pred_out['orig_labels'] = adata.obs[freq_redist]\n",
    "        adata.obs['consensus_clus_prediction'] = pred_out['consensus_clus_prediction']\n",
    "    adata.obs['predicted'] = pred_out['predicted']\n",
    "    adata_temp.obs = adata.obs\n",
    "    \n",
    "    # Estimate top model features for class descrimination\n",
    "    feature_importance = estimate_important_features(model_lr, 100)\n",
    "    mat = feature_importance.euler_pow_mat\n",
    "    top_loadings = feature_importance.to_n_features_long\n",
    "\n",
    "finally: #\n",
    "    current, peak = display_cpu.stop()\n",
    "t1 = time.time()\n",
    "time_s = t1-t0\n",
    "print('projection complete!')\n",
    "time.sleep(3)\n",
    "print('projection time was ' + str(time_s) + ' seconds')\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))\n",
    "\n",
    "# regression summary\n",
    "idx_map = dict(zip(  list(adata.obs[feat_use].unique()),list(range(0,len(list(adata.obs[feat_use].unique()))))))\n",
    "regression_results(adata.obs[feat_use].map(idx_map), adata.obs['predicted'].map(idx_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-bearing",
   "metadata": {},
   "source": [
    "# View by median probabilities per classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_probability_heatmap(pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-involvement",
   "metadata": {},
   "source": [
    "# View by cross-tabulation of two categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_crosstab_heatmap(adata, feat_use, 'predicted')\n",
    "plot_crosstab_heatmap(adata, 'consensus_clus_prediction', 'predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-bridge",
   "metadata": {},
   "source": [
    "# View top predictive features per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimate dataset specific feature impact\n",
    "for classes in ['pDC precursor_ys_HL','AEC_ys_HL']:\n",
    "    model_class_feature_plots(top_loadings, [str(classes)], 'e^coef')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-thomas",
   "metadata": {},
   "source": [
    "# Save predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out.to_csv('./A1_V3_sk_sk_pred_outs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-tongue",
   "metadata": {},
   "source": [
    " # Assess feature impact on model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using a low-dim model like PCA or ldVAE which has a weights layer\n",
    "# top_loadings = compute_weighted_impact(varm_file = '/nfs/team205/ig7/projects/fetal_skin/3_160523_probabillistic_projection_organoid_adt_fetl/A2_V2_ldvae_models/v3_ldvae_obsm_weights.csv',top_loadings =  top_loadings, threshold=0.05)\n",
    "\n",
    "for class_lin in top_loadings['class'].unique():\n",
    "    model_class_feature_plots(top_loadings, [class_lin], 'weighted_impact','e^coef',max_len= 20,title = lineage)\n",
    "    analyze_and_plot(top_loadings,class_lin, max_len=20, pre_ranked=True, database='GO_Biological_Process_2021', cutoff=0.25, min_s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(top_loadings['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_loadings[top_loadings['class'].isin(['Tip cell (arterial)','HSC','SPP1+ proliferating neuron proneitors'])].groupby(['class']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classes in ['Tip cell (arterial)','HSC','SPP1+ proliferating neuron proneitors']:\n",
    "    model_class_feature_plots(top_loadings, [str(classes)], 'e^coef')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-capital",
   "metadata": {},
   "source": [
    "# Label stability scoring for individual label performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "institutional-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_col shape should match the pred_out original labels, so some self-projection works best here\n",
    "pred_col = list(pred_out.columns[pred_out.columns.isin(set(pred_out['orig_labels']))])\n",
    "loss, log_losses, weights = compute_label_log_losses(pred_out, 'orig_labels', pred_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-turkish",
   "metadata": {},
   "source": [
    "# Label confidence scoring, weighted probabilities and label propagation\n",
    "\n",
    "## Bayesian KNN label stability\n",
    "For modelling label uncertainty given neighborhood membership and distances\n",
    "\n",
    "#### Step 1: Generate Binary Neighborhood Membership Matrix\n",
    "The first step is to generate a binary neighborhood membership matrix from the connectivity matrix. This is done with the function get_binary_neigh_matrix(connectivities), which takes a connectivity matrix as input and outputs a binary matrix indicating whether a cell is a neighbor of another cell.\n",
    "\n",
    "The connectivity matrix represents the neighborhood relationships between cells, typically obtained from KNN analysis. In this matrix, each row and column represent a cell, and an entry indicates the 'connectivity' between the corresponding cells.\n",
    "\n",
    "The function transforms the connectivity matrix into a binary matrix by setting all non-zero values to 1, indicating a neighborhood relationship, and all zero values remain as 0, indicating no neighborhood relationship.\n",
    "\n",
    "#### Step 2: Calculate Label Counts\n",
    "Next, the function get_label_counts(neigh_matrix, labels) is used to count the number of occurrences of each label in the neighborhood of each cell. The input to this function is the binary neighborhood membership matrix and a list of labels for each cell.\n",
    "\n",
    "The function returns a matrix in which each row corresponds to a cell, and each column corresponds to a label. Each entry is the count of cells of a particular label in the neighborhood of a given cell.\n",
    "\n",
    "#### Step 3: Compute Distance-Entropy Product\n",
    "In the third step, the function compute_dist_entropy_product(neigh_membership, labels, dist_matrix) computes the product of the average neighborhood distance and the entropy of the label distribution in the neighborhood for each cell and each label.\n",
    "\n",
    "The entropy of a label distribution in a neighborhood is a measure of the diversity or 'mix' of labels in that neighborhood, with higher entropy indicating a more diverse mix of labels. The average neighborhood distance for a cell is the average distance from that cell to all other cells in its neighborhood.\n",
    "\n",
    "By multiplying the entropy with the average distance, this function captures two important aspects of the neighborhood:\n",
    "\n",
    "Entropy: The diversity of labels in a neighborhood. High entropy means the neighborhood is a 'melting pot' of many different labels, while low entropy indicates a neighborhood dominated by a single label.\n",
    "Distance: The spatial proximity of cells in a neighborhood. A high average distance means the cells in a neighborhood are widely dispersed, while a low average distance indicates a compact, closely-knit neighborhood.\n",
    "Thus, the distance-entropy product for a cell provides a measure of the 'stability' of the cell's label, with lower values indicating a stable, consistent label and higher values indicating an unstable, inconsistent label.\n",
    "\n",
    "#### Step 4: Bayesian Sampling and Weight Calculation\n",
    "The final step is the compute_weights function, which uses Bayesian inference to compute a posterior distribution of the distance-entropy product for each label and calculates the weights.\n",
    "\n",
    "In Bayesian inference, we start with a prior distribution that represents our initial belief about the parameter we're interested in, and we update this belief using observed data to get a posterior distribution.\n",
    "\n",
    "In this case, the prior distribution is a normal distribution with mean and standard deviation equal to the mean and standard deviation of the distance-entropy product for the original labels. The observed data is the distance-entropy product for the predicted labels. A normal distribution is a reasonable choice for the prior because the distance-entropy product is a continuous variable that can theoretically take on any real value, and the normal distribution is the most common distribution for such variables.\n",
    "\n",
    "After sampling from the posterior distribution, the weight for each label is calculated as one minus the ratio of the standard deviation of the posterior distribution to the maximum standard deviation across all labels. This means that labels with a larger standard deviation (indicating greater uncertainty about their stability) will have smaller weights, and labels with a smaller standard deviation (indicating less uncertainty) will have larger weights.\n",
    "\n",
    "The weights are returned as a dictionary where each key-value pair corresponds to a label and its weight.\n",
    "\n",
    "#### Step 5: Apply Weights to Probabilities\n",
    "Finally, the weights are applied to the probability dataframe with the function apply_weights(prob_df, weights). The input to this function is a dataframe where each row corresponds to a cell and each column corresponds to a label, with each entry being the probability of the cell being of the label, and a dictionary of weights.\n",
    "\n",
    "This function multiplies each column of the probability dataframe by the corresponding weight, effectively 'boosting' the probabilities of labels with larger weights and 'penalizing' the probabilities of labels with smaller weights. After applying the weights, the function normalizes the probabilities so that they sum to 1 for each cell, returning a dataframe of the same shape as the input but with the probabilities weighted and normalized.\n",
    "\n",
    "Overall, this method provides a principled way to quantify label uncertainty and adjust the probabilities output by a logistic regression model accordingly. It combines the strengths of KNN, which can capture local structure and relationships in the data, and Bayesian inference, which provides a robust framework for dealing with uncertainty and incorporating prior knowledge. By weighting the probabilities according to the stability of the labels, this method can potentially improve the accuracy and interpretability of the logistic regression model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = compute_weights(adata,use_rep = 'neighbors', original_labels_col ='cell.labels', predicted_labels_col = 'cell.labels')\n",
    "adata.obsm['pred_out'] = pred_out\n",
    "adata.obsm['pred_out_weighted'] = apply_weights(adata.obsm['pred_out'],weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-gibson",
   "metadata": {},
   "source": [
    "# Optionally now use the updated probabilities for label propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "novel-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here define new labels with the updated probabilities\n",
    "# Run Freq-redist or 68CI redist amongst neighborhoods or new clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "workhorse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
