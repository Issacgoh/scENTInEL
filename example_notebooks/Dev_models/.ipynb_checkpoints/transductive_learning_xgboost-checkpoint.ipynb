{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "#from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import psutil\n",
    "import random\n",
    "import threading\n",
    "import tracemalloc\n",
    "import itertools\n",
    "import math\n",
    "import warnings\n",
    "import sklearn.metrics as metrics\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "#from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import psutil\n",
    "import random\n",
    "import threading\n",
    "import tracemalloc\n",
    "import itertools\n",
    "import math\n",
    "import warnings\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "import mygene\n",
    "import gseapy as gp\n",
    "import mygene\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import entropy\n",
    "# main_probabillistic_training_projection_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intellectual-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adatas(adatas_dict,data_merge, data_key_use,QC_normalise):\n",
    "    \"\"\"\n",
    "    General description.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if data_merge == True:\n",
    "        # Read\n",
    "        gene_intersect = {} # unused here\n",
    "        adatas = {}\n",
    "        for dataset in adatas_dict.keys():\n",
    "            if 'https' in adatas_dict[dataset]:\n",
    "                print('Loading anndata from web source')\n",
    "                adatas[dataset] = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[dataset])\n",
    "            adatas[dataset] = sc.read(data[dataset])\n",
    "            adatas[dataset].var_names_make_unique()\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            gene_intersect[dataset] = list(adatas[dataset].var.index)\n",
    "        adata = list(adatas.values())[0].concatenate(list(adatas.values())[1:],join='inner')\n",
    "        return adatas, adata\n",
    "    elif data_merge == False:\n",
    "        if 'https' in adatas_dict[data_key_use]:\n",
    "            print('Loading anndata from web source')\n",
    "            adata = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[data_key_use])\n",
    "        else: \n",
    "            adata = sc.read(adatas_dict[data_key_use])\n",
    "    if QC_normalise == True:\n",
    "        print('option to apply standardisation to data detected, performing basic QC filtering')\n",
    "        sc.pp.filter_cells(adata, min_genes=200)\n",
    "        sc.pp.filter_genes(adata, min_cells=3)\n",
    "        sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        \n",
    "    return adata\n",
    "\n",
    "# projection module\n",
    "def reference_projection(adata, model, dyn_std,partial_scale,train_x_partition, **kwargs):\n",
    "    \"\"\"\n",
    "    General description.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    class adata_temp:\n",
    "        pass\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    print('Determining model flavour')\n",
    "    try:\n",
    "        model_lr =  model['Model']\n",
    "        print('Consuming celltypist model')\n",
    "    except:# hasattr(model, 'coef_'):\n",
    "        print('Consuming non-celltypist model')\n",
    "        model_lr =  model\n",
    "    print(model_lr)\n",
    "    if train_x_partition == 'X':\n",
    "        print('Matching reference genes in the model')\n",
    "        k_x = np.isin(list(adata.var.index), list(model_lr.features))\n",
    "        if k_x.sum() == 0:\n",
    "            raise ValueError(f\"ðŸ›‘ No features overlap with the model. Please provide gene symbols\")\n",
    "        print(f\"ðŸ§¬ {k_x.sum()} features used for prediction\")\n",
    "        #slicing adata\n",
    "        k_x_idx = np.where(k_x)[0]\n",
    "        # adata_temp = adata[:,k_x_idx]\n",
    "        adata_temp.var = adata[:,k_x_idx].var\n",
    "        adata_temp.X = adata[:,k_x_idx].X\n",
    "        adata_temp.obs = adata[:,k_x_idx].obs\n",
    "        lr_idx = pd.DataFrame(model_lr.features, columns=['features']).reset_index().set_index('features').loc[list(adata_temp.var.index)].values\n",
    "        # adata_arr = adata_temp.X[:,list(lr_idexes['index'])]\n",
    "        # slice and reorder model\n",
    "        ni, fs, cf = model_lr.n_features_in_, model_lr.features, model_lr.coef_\n",
    "        model_lr.n_features_in_ = lr_idx.size\n",
    "        model_lr.features = np.array(model_lr.features)[lr_idx]\n",
    "        model_lr.coef_ = np.squeeze(model_lr.coef_[:,lr_idx]) #model_lr.coef_[:, lr_idx]\n",
    "        if partial_scale == True:\n",
    "            print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "            # Partial scaling alg\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "            n = adata_temp.X.shape[0]  # number of rows\n",
    "            # set dyn scale packet size\n",
    "            x_len = len(adata_temp.var)\n",
    "            y_len = len(adata.obs)\n",
    "            if y_len < 100000:\n",
    "                dyn_pack = int(x_len/10)\n",
    "                pack_size = dyn_pack\n",
    "            else:\n",
    "                # 10 pack for every 100,000\n",
    "                dyn_pack = int((y_len/100000)*10)\n",
    "                pack_size = int(x_len/dyn_pack)\n",
    "            batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "            index = 0  # helper-var\n",
    "            while index < n:\n",
    "                partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "                partial_x = adata_temp.X[index:index+partial_size]\n",
    "                scaler.partial_fit(partial_x)\n",
    "                index += partial_size\n",
    "            adata_temp.X = scaler.transform(adata_temp.X)\n",
    "    # model projections\n",
    "    print('Starting reference projection!')\n",
    "    if train_x_partition == 'X':\n",
    "        train_x = adata_temp.X\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "        \n",
    "    elif train_x_partition in list(adata.obsm.keys()): \n",
    "        print('{low_dim: this partition modality is still under development!}')\n",
    "        train_x = adata.obsm[train_x_partition]\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "    \n",
    "    else:\n",
    "        print('{this partition modality is still under development!}')\n",
    "    ## insert modules for low dim below\n",
    "\n",
    "    # Simple dynamic confidence calling\n",
    "    pred_out['confident_calls'] = pred_out['predicted']\n",
    "    pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'] = pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'].astype(str) + '_uncertain'\n",
    "    # means_ = self.model.scaler.mean_[lr_idx] if self.model.scaler.with_mean else 0\n",
    "    return(pred_out,train_x,model_lr,adata_temp)\n",
    "\n",
    "# Modified LR train module, does not work with low-dim by default anymore, please use low-dim adapter\n",
    "def LR_train(adata, train_x, train_label, penalty='elasticnet', sparcity=0.2,max_iter=200,l1_ratio =0.2,tune_hyper_params =False,n_splits=5, n_repeats=3,l1_grid = [0.01,0.2,0.5,0.8], c_grid = [0.01,0.2,0.4,0.6], thread_num = -1, **kwargs):\n",
    "    \"\"\"\n",
    "    General description.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if tune_hyper_params == True:\n",
    "        train_labels=train_label\n",
    "        results = tune_lr_model(adata, train_x_partition = train_x, random_state = 42,  train_labels = train_labels, n_splits=n_splits, n_repeats=n_repeats,l1_grid = l1_grid, c_grid = c_grid)\n",
    "        print('hyper_params tuned')\n",
    "        sparcity = results.best_params_['C']\n",
    "        l1_ratio = results.best_params_['l1_ratio']\n",
    "    \n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, n_jobs=thread_num)\n",
    "    if (penalty == \"l1\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual = True, solver = 'liblinear',multi_class = 'ovr', n_jobs=thread_num ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'ovr', n_jobs=thread_num)\n",
    "    if train_x == 'X':\n",
    "        subset_train = adata.obs.index\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#        train_label = train_label[subset_train]\n",
    "        train_x = adata.X#[adata.obs.index.isin(list(adata.obs[subset_train].index))]\n",
    "#        predict_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_predict].index))]\n",
    "    elif train_x in adata.obsm.keys():\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#         train_label = train_label[subset_train]\n",
    "        train_x = adata.obsm[train_x]\n",
    "#        predict_x = train_x\n",
    "#        train_x = train_x[subset_train, :]\n",
    "        # Define prediction parameters\n",
    "#        predict_x = predict_x[subset_predict]\n",
    "#        predict_x = pd.DataFrame(predict_x)\n",
    "#        predict_x.index = adata.obs[subset_predict].index\n",
    "    # Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "    model = lr.fit(train_x, train_label)\n",
    "    model.features = np.array(adata.var.index)\n",
    "    return model\n",
    "\n",
    "def tune_lr_model(adata, train_x_partition = 'X', random_state = 42, use_bayes_opt=True, train_labels = None, n_splits=5, n_repeats=3,l1_grid = [0.1,0.2,0.5,0.8], c_grid = [0.1,0.2,0.4,0.6],thread_num = -1, **kwargs):\n",
    "    \"\"\"\n",
    "    General description.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    import bless as bless\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from numpy import arange\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from skopt import BayesSearchCV\n",
    "\n",
    "    # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "    r = np.random.RandomState(random_state)\n",
    "    if train_x_partition in adata.obsm.keys():\n",
    "        lvg = bless.bless(tune_train_x, RBF(length_scale=20), lam_final = 2, qbar = 2, random_state = r, H = 10, force_cpu=True)\n",
    "    #     try:\n",
    "    #         import cupy\n",
    "    #         lvg_2 = bless(adata.obsm[train_x_partition], RBF(length_scale=10), 10, 10, r, 10, force_cpu=False)\n",
    "    #     except ImportError:\n",
    "    #         print(\"cupy not found, defaulting to numpy\")\n",
    "        adata_tuning = adata[lvg.idx]\n",
    "        tune_train_x = adata_tuning.obsm[train_x_partition][:]\n",
    "    else:\n",
    "        print('no latent representation provided, random sampling instead')\n",
    "        prop = 0.1\n",
    "        random_vertices = []\n",
    "        n_ixs = int(len(adata.obs) * prop)\n",
    "        random_vertices = random.sample(list(range(len(adata.obs))), k=n_ixs)\n",
    "        adata_tuning = adata[random_vertices]\n",
    "        tune_train_x = adata_tuning.X\n",
    "        \n",
    "    if not train_labels == None:\n",
    "        tune_train_label = adata_tuning.obs[train_labels]\n",
    "    elif train_labels == None:\n",
    "        try:\n",
    "            print('no training labels provided, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        except:\n",
    "            print('no training labels provided, no neighbors, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        tune_train_label = adata_tuning.obs['leiden']\n",
    "    ## tune regularization for multinomial logistic regression\n",
    "    print('starting tuning loops')\n",
    "    X = tune_train_x\n",
    "    y = tune_train_label\n",
    "    grid = dict()\n",
    "    # define model\n",
    "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "    #model = LogisticRegression(penalty = penalty, max_iter =  200, dual=False,solver = 'saga', multi_class = 'multinomial',)\n",
    "    model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, n_jobs=thread_num)\n",
    "    if (penalty == \"l1\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, dual = True, solver = 'liblinear',multi_class = 'multinomial', n_jobs=thread_num ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'multinomial', n_jobs=thread_num) # use multinomial class if probabilities are descrete\n",
    "        grid['l1_ratio'] = l1_grid\n",
    "    grid['C'] = c_grid\n",
    "    \n",
    "    if use_bayes_opt == True:\n",
    "        # define search space\n",
    "        search_space = {'C': (np.min(c_grid), np.max(c_grid), 'log-uniform'), \n",
    "                        'l1_ratio': (np.min(l1_grid), np.max(l1_grid), 'uniform') if 'elasticnet' in penalty else None}\n",
    "        # define search\n",
    "        search = BayesSearchCV(model, search_space, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-thread_num)\n",
    "        # perform the search\n",
    "        results = search.fit(X, y)\n",
    "    else:\n",
    "        # define search\n",
    "        search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=thread_num)\n",
    "        # perform the search\n",
    "        results = search.fit(X, y)\n",
    "    # summarize\n",
    "    print('MAE: %.3f' % results.best_score_)\n",
    "    print('Config: %s' % results.best_params_)\n",
    "    return results\n",
    "\n",
    "def prep_training_data(adata_temp,feat_use,batch_key, model_key, batch_correction=False, var_length = 7500,penalty='elasticnet',sparcity=0.2,max_iter = 200,l1_ratio = 0.1,partial_scale=True,train_x_partition ='X',theta = 3,tune_hyper_params=False,thread_num = -1, **kwargs):\n",
    "    \"\"\"\n",
    "    General description.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    model_name = model_key + '_lr_model'\n",
    "    #scale the input data\n",
    "    if partial_scale == True:\n",
    "        print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "        print('performing highly variable gene selection')\n",
    "        sc.pp.highly_variable_genes(adata_temp, batch_key = batch_key, subset=False)\n",
    "        adata_temp = subset_top_hvgs(adata_temp,var_length)\n",
    "        # Partial scaling alg\n",
    "        #adata_temp.X = (adata_temp.X)\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        n = adata_temp.X.shape[0]  # number of rows\n",
    "        # set dyn scale packet size\n",
    "        x_len = len(adata_temp.var)\n",
    "        y_len = len(adata_temp.obs)\n",
    "        if y_len < 100000:\n",
    "            dyn_pack = int(x_len/10)\n",
    "            pack_size = dyn_pack\n",
    "        else:\n",
    "            # 10 pack for every 100,000\n",
    "            dyn_pack = int((y_len/100000)*10)\n",
    "            pack_size = int(x_len/dyn_pack)\n",
    "        batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "        index = 0  # helper-var\n",
    "        while index < n:\n",
    "            partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "            partial_x = adata_temp.X[index:index+partial_size]\n",
    "            scaler.partial_fit(partial_x)\n",
    "            index += partial_size\n",
    "        adata_temp.X = scaler.transform(adata_temp.X)\n",
    "#     else:\n",
    "#         sc.pp.scale(adata_temp, zero_center=True, max_value=None, copy=False, layer=None, obsm=None)\n",
    "    if (train_x_partition != 'X') & (train_x_partition in adata_temp.obsm.keys()):\n",
    "        print('train partition is not in OBSM, defaulting to PCA')\n",
    "        print('performing highly variable gene selection')\n",
    "        sc.pp.highly_variable_genes(adata_temp, batch_key = batch_key, subset=False)\n",
    "        adata_temp = subset_top_hvgs(adata_temp,var_length)\n",
    "        # Now compute PCA\n",
    "        sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "        sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "        \n",
    "        # Batch correction options\n",
    "        # The script will test later which Harmony values we should use \n",
    "        if(batch_correction == \"Harmony\"):\n",
    "            print(\"Commencing harmony\")\n",
    "            adata_temp.obs['lr_batch'] = adata_temp.obs[batch_key]\n",
    "            batch_var = \"lr_batch\"\n",
    "            # Create hm subset\n",
    "            adata_hm = adata_temp[:]\n",
    "            # Set harmony variables\n",
    "            data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "            meta_data = adata_hm.obs\n",
    "            vars_use = [batch_var]\n",
    "            # Run Harmony\n",
    "            ho = hm.run_harmony(data_mat, meta_data, vars_use,theta=theta)\n",
    "            res = (pd.DataFrame(ho.Z_corr)).T\n",
    "            res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "            # Insert coordinates back into object\n",
    "            adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "            adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "            # Run neighbours\n",
    "            #sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            adata_temp = adata_hm[:]\n",
    "            del adata_hm\n",
    "        elif(batch_correction == \"BBKNN\"):\n",
    "            print(\"Commencing BBKNN\")\n",
    "            sc.external.pp.bbknn(adata_temp, batch_key=batch_var, approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=15) \n",
    "        print(\"adata1 and adata2 are now combined and preprocessed in 'adata' obj - success!\")\n",
    "\n",
    "\n",
    "    # train model\n",
    "#    train_x = adata_temp.X\n",
    "    #train_label = adata_temp.obs[feat_use]\n",
    "    print('proceeding to train model')\n",
    "    model = LR_train(adata_temp, train_x = train_x_partition, train_label=feat_use, penalty=penalty, sparcity=sparcity,max_iter=max_iter,l1_ratio = l1_ratio,tune_hyper_params = tune_hyper_params)\n",
    "    model.features = list(adata_temp.var.index)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-spyware",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accredited-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection module\n",
    "def reference_projection(adata,model,train_x_partition, **kwargs):\n",
    "    \"\"\"\n",
    "    General description.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    class adata_temp:\n",
    "        pass\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    print('Determining model flavour')\n",
    "    try:\n",
    "        model_lr =  model['Model']\n",
    "        print('Consuming celltypist model')\n",
    "    except:# hasattr(model, 'coef_'):\n",
    "        print('Consuming non-celltypist model')\n",
    "        model_lr =  model\n",
    "    print(model_lr)\n",
    "    if train_x_partition == 'X':\n",
    "        print('Matching reference genes in the model')\n",
    "        k_x = np.isin(list(adata.var.index), list(model_lr.features))\n",
    "        if k_x.sum() == 0:\n",
    "            raise ValueError(f\"ðŸ›‘ No features overlap with the model. Please provide gene symbols\")\n",
    "        print(f\"ðŸ§¬ {k_x.sum()} features used for prediction\")\n",
    "        #slicing adata\n",
    "        k_x_idx = np.where(k_x)[0]\n",
    "        # adata_temp = adata[:,k_x_idx]\n",
    "        adata_temp.var = adata[:,k_x_idx].var\n",
    "        adata_temp.X = adata[:,k_x_idx].X\n",
    "        adata_temp.obs = adata[:,k_x_idx].obs\n",
    "        lr_idx = pd.DataFrame(model_lr.features, columns=['features']).reset_index().set_index('features').loc[list(adata_temp.var.index)].values\n",
    "        # adata_arr = adata_temp.X[:,list(lr_idexes['index'])]\n",
    "        # slice and reorder model\n",
    "        ni, fs, cf = model_lr.n_features_in_, model_lr.features, model_lr.coef_\n",
    "        model_lr.n_features_in_ = lr_idx.size\n",
    "        model_lr.features = np.array(model_lr.features)[lr_idx]\n",
    "        model_lr.coef_ = np.squeeze(model_lr.coef_[:,lr_idx]) #model_lr.coef_[:, lr_idx]\n",
    "        if 'partial_scale' in kwargs:\n",
    "            if partial_scale == True:\n",
    "                print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "                # Partial scaling alg\n",
    "                scaler = StandardScaler(with_mean=False)\n",
    "                n = adata_temp.X.shape[0]  # number of rows\n",
    "                # set dyn scale packet size\n",
    "                x_len = len(adata_temp.var)\n",
    "                y_len = len(adata.obs)\n",
    "                if y_len < 100000:\n",
    "                    dyn_pack = int(x_len/10)\n",
    "                    pack_size = dyn_pack\n",
    "                else:\n",
    "                    # 10 pack for every 100,000\n",
    "                    dyn_pack = int((y_len/100000)*10)\n",
    "                    pack_size = int(x_len/dyn_pack)\n",
    "                batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "                index = 0  # helper-var\n",
    "                while index < n:\n",
    "                    partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "                    partial_x = adata_temp.X[index:index+partial_size]\n",
    "                    scaler.partial_fit(partial_x)\n",
    "                    index += partial_size\n",
    "                adata_temp.X = scaler.transform(adata_temp.X)\n",
    "    # model projections\n",
    "    print('Starting reference projection!')\n",
    "    if train_x_partition == 'X':\n",
    "        train_x = adata_temp.X\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "        \n",
    "    elif train_x_partition in list(adata.obsm.keys()): \n",
    "        print('{low_dim: this partition modality is still under development!}')\n",
    "        train_x = adata.obsm[train_x_partition]\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "    \n",
    "    else:\n",
    "        print('{this partition modality is still under development!}')\n",
    "    ## insert modules for low dim below\n",
    "\n",
    "    # Simple dynamic confidence calling\n",
    "    pred_out['confident_calls'] = pred_out['predicted']\n",
    "    pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'] = pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'].astype(str) + '_uncertain'\n",
    "    # means_ = self.model.scaler.mean_[lr_idx] if self.model.scaler.with_mean else 0\n",
    "    return(pred_out,train_x,model_lr,adata_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "actual-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "'pan_fetal_wget':'https://celltypist.cog.sanger.ac.uk/models/Pan_Fetal_Suo/v2/Pan_Fetal_Human.pkl',\n",
    "'YS_wget':'https://storage.googleapis.com/haniffalab/yolk-sac/YS_X_A2_V12_lvl3_ELASTICNET_YS.sav',\n",
    "}\n",
    "\n",
    "adatas_dict = {\n",
    "'pan_fetal_wget':'https://cellgeni.cog.sanger.ac.uk/developmentcellatlas/fetal-immune/PAN.A01.v01.raw_count.20210429.PFI.embedding.h5ad',\n",
    "'YS_wget':'https://app.cellatlas.io/yolk-sac/dataset/23/download',\n",
    "'FLIV_wget':'https://app.cellatlas.io/fetal-liver/dataset/1/download'\n",
    "}\n",
    "\n",
    "# Variable assignment\n",
    "train_model = False\n",
    "feat_use = 'cell.labels'\n",
    "adata_key = 'FLIV_wget'#'fliv_wget_test' # key for dictionary entry containing local or web path to adata/s can be either url or local \n",
    "data_merge = False # read and merge multiple adata (useful, but keep false for now)\n",
    "model_key = 'pan_fetal_wget'#'test_low_dim_ipsc_ys'# key for model of choice can be either url or local \n",
    "train_x_partition = 'X' # what partition was the data trained on? To keep simple, for now only accepts 'X'\n",
    "dyn_std = 1.96 # Dynamic cutoffs using std of the mean for each celltype probability, gives a column notifying user of uncertain labels 1 == 68Ci, 1.96 = 95CI\n",
    "freq_redist = 'cell.labels'#'cell.labels'#'False#'cell.labels'#False # False or key of column in anndata object which contains labels/clusters // not currently implemented\n",
    "QC_normalise = True # should data be normalised?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-australian",
   "metadata": {},
   "source": [
    "# Read in query data for projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "supposed-trance",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3348152279.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [7]\u001b[0;36m\u001b[0m\n\u001b[0;31m    adata =  scent.\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# setup model parameters:\n",
    "model_params = {\n",
    "'penalty':'elasticnet', # can be [\"l1\",\"l2\",\"elasticnet\"],\n",
    "'var_length' : 7500,\n",
    "'batch_key' : 'donor',\n",
    "'sparcity' : 0.5, #If using LR without optimisation, this controls the sparsity in model C penalty for degree of regularisation\n",
    "'max_iter' : 1000, #Increase if experiencing max iter issues\n",
    "'thread_num' : -1,\n",
    "'l1_ratio' : 0.5, #If using elasticnet without optimisation, this controls the ratio between l1 and l2)\n",
    "'partial_scale' : False, # should data be scaled in batches?\n",
    "'tune_hyper_params' : True, # Current implementation is very expensive, intentionally made rigid for now\n",
    "'batch_correction' : False,\n",
    "}\n",
    "\n",
    "if train_model == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    adata =  scent.\n",
    "    \n",
    "    (adatas_dict, data_merge, adata_key, QC_normalise)\n",
    "    print('adata_loaded')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    display_cpu = scent.DisplayCPU()\n",
    "    display_cpu.start()\n",
    "    try:\n",
    "        model_trained = scent.prep_training_data(feat_use = feat_use,\n",
    "        adata_temp = adata,\n",
    "        train_x_partition = train_x_partition,\n",
    "        model_key = model_key + '_lr_model',\n",
    "        **model_params\n",
    "        )\n",
    "        filename =model_name\n",
    "        pkl.dump(model_trained, open(filename, 'wb'))\n",
    "    finally: #\n",
    "        current, peak = display_cpu.stop()\n",
    "        t1 = time.time()\n",
    "        time_s = t1-t0\n",
    "        print('training complete!')\n",
    "        time.sleep(3)\n",
    "        print('projection time was ' + str(time_s) + ' seconds')\n",
    "        print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "        print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "        print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "        print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))\n",
    "    model_lr= model_trained\n",
    "    adata =  scent.load_adatas(adatas_dict, data_merge, adata_key)\n",
    "else:\n",
    "    adata =  scent.load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "    model = scent.load_models(models,model_key)\n",
    "    model_lr =  model\n",
    "    \n",
    "# run with usage logger\n",
    "import time\n",
    "t0 = time.time()\n",
    "display_cpu = scent.DisplayCPU()\n",
    "display_cpu.start()\n",
    "try: #code here ##\n",
    "    pred_out,train_x,model_lr,adata_temp = scent.reference_projection(adata, model_lr, dyn_std,partial_scale,train_x_partition, **model_params)\n",
    "    if freq_redist != False:\n",
    "        pred_out['orig_labels'] = adata.obs[freq_redist]\n",
    "        pred_out = scent.freq_redist_68CI(pred_out,'orig_labels')\n",
    "        adata.obs['consensus_clus_prediction'] = pred_out['consensus_clus_prediction']\n",
    "    adata.obs['predicted'] = pred_out['predicted']\n",
    "    adata_temp.obs = adata.obs\n",
    "    \n",
    "    # Estimate top model features for class descrimination\n",
    "    feature_importance = scent.estimate_important_features(model_lr, 100)\n",
    "    mat = feature_importance.euler_pow_mat\n",
    "    top_loadings = feature_importance.to_n_features_long\n",
    "\n",
    "finally: #\n",
    "    current, peak = display_cpu.stop()\n",
    "t1 = time.time()\n",
    "time_s = t1-t0\n",
    "print('projection complete!')\n",
    "time.sleep(3)\n",
    "print('projection time was ' + str(time_s) + ' seconds')\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model == True:\n",
    "    train_label = adata.obs[feat_use].values\n",
    "else:\n",
    "    train_label = adata.obs['predicted'].values\n",
    "scent.report_f1(model_lr,train_x, train_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scentinel",
   "language": "python",
   "name": "scneitnel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
