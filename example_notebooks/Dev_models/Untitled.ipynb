{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "detected-surfing",
   "metadata": {},
   "source": [
    "# SGD_pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-voice",
   "metadata": {},
   "source": [
    "### **Preprocessing for PageRank on Single Cell Data**\n",
    "\n",
    "### **1. Graph Representation**\n",
    "\n",
    "Represent the graph by its adjacency matrix \\( A \\), where \\( A_{ij} \\) represents the weight of the edge from node \\( i \\) to node \\( j \\). If there's no edge between \\( i \\) and \\( j \\), \\( A_{ij} = 0 \\).\n",
    "\n",
    "### **2. Degree Calculation**\n",
    "\n",
    "The degree \\( D_i \\) of a node \\( i \\) is the sum of the weights of its edges:\n",
    "\n",
    "$$\n",
    "D_i = \\sum_{j} A_{ij}\n",
    "$$\n",
    "\n",
    "### **3. Graph Normalization**\n",
    "\n",
    "Given the adjacency matrix \\( A \\) derived from single cell data, it's crucial to normalize this matrix to ensure that the computation is not unduly influenced by nodes (cells) with higher degrees. Two primary normalization techniques can be applied:\n",
    "\n",
    "#### **A. Normalized Adjacency Matrix**\n",
    "\n",
    "For each node \\( i \\), compute the inverse square root of its degree:\n",
    "\n",
    "$$\n",
    "S_i = \\frac{1}{\\sqrt{D_i}}\n",
    "$$\n",
    "\n",
    "Modify the adjacency matrix \\( A \\) using these values:\n",
    "\n",
    "$$\n",
    "M_1 = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( A \\) is the adjacency matrix.\n",
    "- \\( D \\) is the diagonal matrix with node degrees.\n",
    "\n",
    "**Rationale**:\n",
    "- This normalization emphasizes nodes that have meaningful connections, i.e., nodes that are connected to other nodes with high degrees.\n",
    "- Suitable for situations where you want to capture the core structure of the graph, emphasizing stronger connections.\n",
    "\n",
    "#### **B. Modified Normalized Laplacian**\n",
    "\n",
    "$$\n",
    "L_{\\text{norm}} = I + D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( I \\) is the identity matrix.\n",
    "\n",
    "**Rationale**:\n",
    "- The normalized Laplacian captures the difference between a node's isolated state and its actual connections.\n",
    "- Suitable for spectral clustering and other methods that require understanding the difference in connectivity patterns.\n",
    "\n",
    "### **4. PageRank Computation**\n",
    "\n",
    "Once the matrix is normalized, we can compute the PageRank. Two main computation techniques can be applied:\n",
    "\n",
    "#### **A. Traditional PageRank**\n",
    "\n",
    "The PageRank vector \\( v \\) is iteratively updated using:\n",
    "\n",
    "$$\n",
    "v_{\\text{new}} = d \\times M v_{\\text{old}} + \\frac{(1 - d)}{N}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( d \\) is the damping factor.\n",
    "- \\( N \\) is the total number of nodes.\n",
    "- \\( M \\) can be either \\( M_1 \\) or \\( L_{\\text{norm}} \\), depending on the chosen normalization.\n",
    "\n",
    "**Rationale**:\n",
    "- Direct and comprehensive: Every iteration updates using the entire graph.\n",
    "- Suitable when computational resources are not a constraint and when we want a deterministic outcome.\n",
    "\n",
    "#### **B. SGD-PageRank**\n",
    "\n",
    "The iterative update equation for a mini-batch is:\n",
    "\n",
    "$$\n",
    "v_{\\text{mini\\_batch}} = d \\times (\\text{learning\\_rate} \\times M_{\\text{mini\\_batch}} v) + \\frac{(1 - d)}{N}\n",
    "$$\n",
    "\n",
    "**Rationale**:\n",
    "- Stochastic Gradient Descent (SGD) approach is more scalable and can handle large graphs efficiently by only using a subset of the graph in each iteration.\n",
    "- Suitable for very large graphs where traditional PageRank may be computationally intensive.\n",
    "- Provides a level of randomness which might help escape local minima.\n",
    "- Two sampling methods are proposed: `probability_based` (where nodes less visited have a higher chance of being selected) and `cyclic` (nodes are visited in a cycle).\n",
    "\n",
    "### **5. Convergence**\n",
    "\n",
    "Convergence in the SGD approach is determined by monitoring the L2 norm between the current and previous PageRank vectors. A smoothed L2 norm is also calculated to detect 'dips' in the convergence curve, which can signal phases where the model begins to learn the graph's structure.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "The proposed method provides a comprehensive framework for preprocessing single cell data and computing PageRank scores. By offering options at each step, it allows users to tailor the approach to their specific dataset and computational constraints. The method capitalizes on the strengths of both traditional and SGD-based PageRank algorithms, ensuring scalability without compromising on the accuracy of the derived rankings.\n",
    "\n",
    "- Ergodan theory of markov chains -- we will always approach a stationary state of a transition matrix if we keep multiplyin against a set of random integers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "workhorse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
